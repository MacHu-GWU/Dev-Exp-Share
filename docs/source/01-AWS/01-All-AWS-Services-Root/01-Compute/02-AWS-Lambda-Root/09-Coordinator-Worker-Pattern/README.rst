.. _coordinator-worker-pattern:

Coordinator Worker 协调者 工作者 模式
==============================================================================
Keywords:


什么是 Coordinator Worker 模式
------------------------------------------------------------------------------
Coordinator Worker (CW) 是一种设计模式, 适用于解决工作量庞大, 且可能出错的任务.

这样的应用场景有很多, 例如:

1. 一个爬虫要从很多网页中抓取数据, 网页数量可能是百万级, 而且抓取数据的逻辑可能出错, 也可能根本页面就没有响应.
2. 电商平台的队列里出现了很多订单, 每个订单只允许被精确处理一次, 并且可能因为各种原因例如支付方式, 库存不够等导致处理失败.
3. 我们需要异步处理 1000 个数据文件, 每个文件处理的时间较长, 在 1-5 分钟之间, 而且有可能因为格式错误, 数据错误导致失败.


面临的主要挑战
------------------------------------------------------------------------------
我们来考虑一下主要的挑战:

1. 任务量庞大, 意味着单个 worker 可能耗时非常长才能完成任务. 我们希望能用很多 worker 同时进行工作, 提高工作效率.
2. 整个工作的任务很多, 每个任务都可能出错, 我们希望保证所有的任务都能被正确完成. 而有的任务因为外部原因根本不可能完成, 我们不希望无限制的浪费资源在这些不可能完成的任务上.
3. 我们希望每个任务被执行仅一次, 不是多次, 也不是 0 次, 而是精确一次.


传统的消息队列方案
------------------------------------------------------------------------------
业内比较传统的做法是使用消息队列, 把任务推送到消息队列中然后触发多个 worker 进行工作. 但这么做很依赖于前期设计, 稍有不慎就会导致消息乱掉, 发生重复消费和丢数据. 一旦出错, 是很难确定哪个消息被消费过, 哪个没有被消费过.

所以要想比较精确的保证每个任务被成功处理一次, 这就需要引入数据库作为分布式一致性保证. 这里我们用的是 AWS Dynamodb 数据库来实现, 因为 Dynamodb 是 Key Value Store, 并且自动伸缩, 分布式高并发低延迟. 实际上有很多 backend 数据库都能实现这一点, 我们只是用它做个例子, 这不是 CW 的核心.


理解 CW 模式是如何解决问题的
------------------------------------------------------------------------------
下面我们来看一下 CW 模式是怎么解决这一问题的.

**首先我们要定义几个概念**:

- **T (Task)**: 任务的最小单位, 每个任务彼此之间独立. 一个任务可以是 "从单个网页中提取数据", "处理一个电商订单", "处理一个数据文件".
- **C (Coordinator)**: 协调者, 本身不处理 T, 主要负责监控每个 T 的状态, 以及给 Worker 派任务.
- **W (Worker)**: 工作者, 可以有很多个彼此独立的 W, 主要负责处理每个 T.

**每个 Task 有这么几个 Attribute (属性)**:

- Key: 用于唯一确定一个 T
- Status: 0 ~ 100, 用于追踪 T 的完成状态, 其中有下面几个状态:
    - todo = 0: 任务被标记成可追踪, 但没有被处理过
    - failed - 20: 任务失败
    - processing = 30: 正在处理中
    - finished = 50: 已被成功处理
    - ignore = 70: 由于失败很多次了, 或是其他原因, 我们不想再处理这个任务了
- Update Time: 时间戳, 记录了上一次任务状态被更新的时间
- Locked: 布尔值, 用于锁定一个 T, 防止 double consume (多重消费)
- Last Lock Time: 时间戳, 记录上一次 Lock 的时间, 防止一个 Task 因为以外被无限锁死. 如果一个锁的时间戳已经超过了很长时间 (例如 1 天), 那么自动释放锁
- Retry: 记录尝试次数, 如果已经超过 K 次依然不成功, 就没有必要再尝试了, 将 Status 设定为 ignore_by_error 以供以后复查
- Data: 与 T 有关的详细数据, 取决于不同的业务

**而每个 Worker 的处理逻辑是这样的**:

- 开始前先检查锁, 如果已经被锁上了则直接结束. 如果没有则将其锁上, 进行真正的业务逻辑, 并且将 Status 设为 processing
- 执行底层实际的业务逻辑, 争取捕获各种已知的异常.
- 顶层逻辑是对底层逻辑的封装, 如果底层逻辑被成功执行, 则将 Status 设为 finished, 然后释放锁.
- 如果底层逻辑执行失败, 并且 retry + 1, 如果 retry 已经超过了阈值, 则将 Status 设为 ignore, 反之则设置为 failed.
- 顶层逻辑对底层逻辑进行 try except 的封装, 无论是成功还是失败, 都要释放锁. 即使连这一步都失败, 我们的锁依然可以在一定时间后失效.

**最后 Coordinator 的处理逻辑是这样的**:

- 从数据库中根据 status code 找到那些未完成的 task, 不要找出所有的, 按照一定规则 (例如根据时间顺序) 取出 Task, 检查他们的 Lock, 只把没有被 lock 的 task 发送给 worker.
- 定期接受新的任务, 并将他们初始化, 将 status 设为 todo.

**总结**

根据以上流程可以看出:

- Coordinator 只会将 "没有完成, 没有被锁住" 的 Task 交给 Worker, 避免了重复消费.
- 如果 retry 超过一定阈值, 则自动被设为 ignore, 避免了无限卡死循环.
- Coordinator 可以用参数定义多久派遣一次任务, 以及一次派遣多少个任务, 实现了灵活的并发控制, 可以控制处理 Task 的速度的快慢.
- 数据库中有 Status 和 Update Time 的索引, 可以高效的取出特定的 Status 的任务, 并且按照时间排序.
- 基于数据库我们可以随时停止这个系统, 也可以随时对失败的数据, ignore 的数据, 成功的数据进行检查, 后续处理. 甚至可以在数据库中记录统计数据信息, 然后进行实时的 monitor.

而这一套方案其实是可以使用任何系统来作为 Coordinator, Worker 以及自行选择 backend database.

Coordinator 和 Worker 可以是 虚拟机, 微服务, 容器, AWS EC2, AWS Lambda 等等. 而 backend database 可以是 SQL, NOSQL, Key Value, AWS Dynamodb, Postgres, Mongodb 都可以.


示例 Python 代码实现
------------------------------------------------------------------------------
.. literalinclude:: ./coordinator_worker_pattern.py
   :language: python
