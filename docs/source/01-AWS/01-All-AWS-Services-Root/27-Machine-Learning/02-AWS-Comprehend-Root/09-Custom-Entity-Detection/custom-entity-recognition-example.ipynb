{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Amazon Comprehend - Custom Entity Detection Example\n",
    "\n",
    "Reference:\n",
    "\n",
    "- [Custom entity recognition](https://docs.aws.amazon.com/comprehend/latest/dg/custom-entity-recognition.html)\n",
    "\n",
    "## 1. Create Fake Dataset\n",
    "\n",
    "In this section, we create a dummy dataset that includes two important entities:\n",
    "\n",
    "- Service Date (target entity)\n",
    "- Receive Date (distracter entity)\n",
    "\n",
    "Each document has several paragraph, and each paragraph has several sentence. There are always one sentence with ``service date`` information and one sentence with ``receive date`` information.\n",
    "\n",
    "In this example, we use the [Plain-text annotations](https://docs.aws.amazon.com/comprehend/latest/dg/cer-annotation-csv.html) format for annotation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "import typing as T\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "from faker import Faker\n",
    "from s3pathlib import S3Path, context\n",
    "from boto_session_manager import BotoSesManager\n",
    "from rich import print as rprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # aws account related\n",
    "    aws_profile = \"aws_data_lab_sanhe_us_east_1\"\n",
    "    s3_bucket = \"669508176277-us-east-1-data\"\n",
    "    comprehend_iam_role = \"arn:aws:iam::669508176277:role/sanhe-comprehend-admin-access\"\n",
    "\n",
    "    # comprehend model / dataset related\n",
    "    model_name = \"medical-service-report-entities\"\n",
    "    n_document = 200\n",
    "    n_paragraph_per_doc_lower = 3\n",
    "    n_paragraph_per_doc_upper = 6\n",
    "    n_sentence_per_paragraph_lower = 3\n",
    "    n_sentence_per_paragraph_upper = 10\n",
    "    n_word_per_sentence_lower = 6\n",
    "    n_word_per_sentence_upper = 20\n",
    "\n",
    "    service_date_key_options = [\"service date\", \"date of service\"]\n",
    "    receive_date_key = \"receive date\"\n",
    "    date_format_options = [\"%Y-%m-%d\", \"%m/%d/%Y\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "bsm = BotoSesManager(profile_name=Config.aws_profile)\n",
    "context.attach_boto_session(bsm.boto_ses)\n",
    "\n",
    "fake = Faker()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Likely myself reach turn story.', 'Environmental choice challenge senior tough manage.', 'Husband once especially return including truth.', 'Owner mention human little should energy.']\n"
     ]
    }
   ],
   "source": [
    "def create_initial_sentences() -> T.List[str]:\n",
    "    n_sentence = random.randint(\n",
    "        Config.n_sentence_per_paragraph_lower,\n",
    "        Config.n_sentence_per_paragraph_upper,\n",
    "    )\n",
    "    return [\n",
    "        fake.sentence(\n",
    "            nb_words=random.randint(\n",
    "                Config.n_word_per_sentence_lower,\n",
    "                Config.n_word_per_sentence_upper,\n",
    "            ),\n",
    "            variable_nb_words=False,\n",
    "        )\n",
    "        for _ in range(n_sentence)\n",
    "    ]\n",
    "\n",
    "print(create_initial_sentences())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summer strong social. Strong three forget power education measure. Scientist here kitchen power short bill. Enter now camera answer administration.\n"
     ]
    }
   ],
   "source": [
    "def create_paragraph() -> str:\n",
    "    return \" \".join(create_initial_sentences())\n",
    "\n",
    "print(create_paragraph())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Son president wind eye. Without law around. Pattern international character. author service date call 1984-12-14 growth understand animal serve. North whatever machine maybe never there.', 102, 112)\n"
     ]
    }
   ],
   "source": [
    "def create_important_paragraph(key: str) -> T.Tuple[str, int, int]:\n",
    "    sentence_list = create_initial_sentences()\n",
    "    n_word = random.randint(\n",
    "        Config.n_word_per_sentence_lower,\n",
    "        Config.n_word_per_sentence_upper,\n",
    "    )\n",
    "    words = [fake.word() for _ in range(n_word)]\n",
    "\n",
    "    # IMPORTANT, the logic below calculates the begin offset and end offset\n",
    "    value = datetime.strptime(fake.date(), \"%Y-%m-%d\").strftime(random.choice(Config.date_format_options))\n",
    "\n",
    "    key_index = random.randint(1, n_word)\n",
    "    words.insert(key_index, key)\n",
    "\n",
    "    value_index = random.randint(key_index+1, n_word+1)\n",
    "    words.insert(value_index, value)\n",
    "\n",
    "    sentence = \" \".join(words) + \".\"\n",
    "    begin_offset = len(\" \".join(words[:value_index])) + 1\n",
    "    end_offset = begin_offset + len(value)\n",
    "\n",
    "    n_sentence = len(sentence_list)\n",
    "    sentence_index = random.randint(1, n_sentence)\n",
    "    sentence_list.insert(sentence_index, sentence)\n",
    "    sentence_offset = len(\" \".join(sentence_list[:sentence_index])) + 1\n",
    "    paragraph = \" \".join(sentence_list)\n",
    "\n",
    "    begin_offset += sentence_offset\n",
    "    end_offset += sentence_offset\n",
    "\n",
    "    # debug\n",
    "    # print([paragraph[begin_offset-1], paragraph[begin_offset], paragraph[begin_offset+1]])\n",
    "    # print([paragraph[end_offset-1], paragraph[end_offset], paragraph[end_offset+1]])\n",
    "\n",
    "    return paragraph, begin_offset, end_offset\n",
    "\n",
    "print(create_important_paragraph(\"service date\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Charge culture become PM. Already certainly child agent. Serious as case show development drive.\\nTell conference all meet. Impact small value any. example date of service police 1988-01-15 according human marriage. Rich four prevent high street investment. Per current security price loss.\\nMonth hold serious. Television exist reality eight thing. Inside citizen enjoy college hear. Event laugh join. Form street ok after billion.\\nStation simply be phone. current receive date land 1974-10-06 very find small. Congress plant which up. Fight cost late stuff. Sell fund identify perform century.\\nBad story huge fine. Woman describe continue able myself well. Suffer question design. Study sport argue.', 178, 188, 482, 492)\n"
     ]
    }
   ],
   "source": [
    "def create_document() -> T.Tuple[str, int, int, int, int]:\n",
    "    n_paragraph = random.randint(Config.n_paragraph_per_doc_lower, Config.n_paragraph_per_doc_upper)\n",
    "    paragraph_list = [create_paragraph() for _ in range(n_paragraph)]\n",
    "\n",
    "    (\n",
    "        service_date_paragraph,\n",
    "        service_date_begin_offset,\n",
    "        service_date_end_offset,\n",
    "    )= create_important_paragraph(key=random.choice(Config.service_date_key_options))\n",
    "    (\n",
    "        receive_date_paragraph,\n",
    "        receive_date_begin_offset,\n",
    "        receive_date_end_offset,\n",
    "    )= create_important_paragraph(key=Config.receive_date_key)\n",
    "\n",
    "    # IMPORTANT, insert the important paragraph to document and\n",
    "    # calculate the line offset\n",
    "    index1_and_index2 = random.sample(list(range(1, n_paragraph)), 2)\n",
    "    index1_and_index2.sort()\n",
    "    index1, index2 = index1_and_index2\n",
    "\n",
    "    paragraph_list.insert(index2, receive_date_paragraph)\n",
    "    delta = len(\"\\n\".join(paragraph_list[:index2])) + 1\n",
    "    receive_date_begin_offset += (delta + len(service_date_paragraph) + 1)\n",
    "    receive_date_end_offset += (delta + len(service_date_paragraph) + 1)\n",
    "\n",
    "    paragraph_list.insert(index1, service_date_paragraph)\n",
    "    delta = len(\"\\n\".join(paragraph_list[:index1])) + 1\n",
    "    service_date_begin_offset += delta\n",
    "    service_date_end_offset += delta\n",
    "\n",
    "    document = \"\\n\".join(paragraph_list)\n",
    "\n",
    "    # debug\n",
    "    # print([document[service_date_begin_offset-1], document[service_date_begin_offset], document[service_date_begin_offset+1]])\n",
    "    # print([document[service_date_end_offset-1], document[service_date_end_offset], document[service_date_end_offset+1]])\n",
    "    # print([document[receive_date_begin_offset-1], document[receive_date_begin_offset], document[receive_date_begin_offset+1]])\n",
    "    # print([document[receive_date_end_offset-1], document[receive_date_end_offset], document[receive_date_end_offset+1]])\n",
    "\n",
    "    return (\n",
    "        document,\n",
    "        service_date_begin_offset,\n",
    "        service_date_end_offset,\n",
    "        receive_date_begin_offset,\n",
    "        receive_date_end_offset,\n",
    "    )\n",
    "\n",
    "print(create_document())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Write Training / Testing to S3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preview s3dir_document: https://console.aws.amazon.com/s3/buckets/669508176277-us-east-1-data?prefix=poc/2023-04-09-custom-document-classification-example/documents/\n"
     ]
    }
   ],
   "source": [
    "# Define S3 locations\n",
    "s3dir_project = S3Path.from_s3_uri(f\"s3://{Config.s3_bucket}/poc/2023-04-09-custom-document-classification-example/\")\n",
    "s3dir_doc = s3dir_project.joinpath(\"documents\").to_dir()\n",
    "s3dir_doc_train = s3dir_doc.joinpath(\"train\").to_dir()\n",
    "s3dir_doc_test = s3dir_doc.joinpath(\"test\").to_dir()\n",
    "s3path_annotation_train = s3dir_project.joinpath(\"annotation-train.csv\")\n",
    "s3path_annotation_test = s3dir_project.joinpath(\"annotation-test.csv\")\n",
    "\n",
    "print(f\"preview s3dir_document: {s3dir_doc.console_url}\")\n",
    "\n",
    "# _ = s3dir_project.delete_if_exists()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Create dataset and split into train and test (70/30)\n",
    "training = list()\n",
    "testing = list()\n",
    "for ith_doc in range(1, 1+Config.n_document):\n",
    "    doc_tuple = create_document()\n",
    "    filename = f\"{str(ith_doc).zfill(6)}.txt\"\n",
    "    sample = (filename, doc_tuple)\n",
    "    if random.randint(1, 100) <= 70:\n",
    "        training.append(sample)\n",
    "    else:\n",
    "        testing.append(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "# Write data and annotation to S3\n",
    "for s3dir_document, s3path_annotation, samples in [\n",
    "    (s3dir_doc_train, s3path_annotation_train, training),\n",
    "    (s3dir_doc_test, s3path_annotation_test, testing),\n",
    "]:\n",
    "    annotation_rows = list()\n",
    "    for (\n",
    "        filename,\n",
    "        (\n",
    "            document,\n",
    "            service_date_begin_offset,\n",
    "            service_date_end_offset,\n",
    "            receive_date_begin_offset,\n",
    "            receive_date_end_offset,\n",
    "        ),\n",
    "    ) in samples:\n",
    "        s3path = s3dir_document.joinpath(filename)\n",
    "        s3path.write_text(document)\n",
    "        annotation_rows.append((\n",
    "            filename,\n",
    "            service_date_begin_offset,\n",
    "            service_date_end_offset,\n",
    "            \"SERVICE_DATE\",\n",
    "        ))\n",
    "        annotation_rows.append((\n",
    "            filename,\n",
    "            receive_date_begin_offset,\n",
    "            receive_date_end_offset,\n",
    "            \"RECEIVE_DATE\",\n",
    "        ))\n",
    "    annotation_df = pl.DataFrame(\n",
    "        annotation_rows,\n",
    "        schema=[\"File\", \"Begin Offset\", \"End Offset\", \"Type\"],\n",
    "    )\n",
    "    with s3path_annotation.open(\"wb\") as f:\n",
    "        annotation_df.write_csv(f, has_header=True, separator=\",\")\n",
    "    #     break\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Train Comprehend Model\n",
    "\n",
    "In this example, we use [comprehend.create_entity_recognizer](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend/client/create_entity_recognizer.html) API to create a custom entity recognizer.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "{'EntityRecognizerArn': 'arn:aws:comprehend:us-east-1:669508176277:entity-recognizer/medical-service-report-entities/version/v000002',\n 'ResponseMetadata': {'RequestId': 'c7352d2d-691c-4813-9584-64556870a90f',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'c7352d2d-691c-4813-9584-64556870a90f',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '133',\n   'date': 'Mon, 10 Apr 2023 05:30:10 GMT'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsm.comprehend_client.create_entity_recognizer(\n",
    "    RecognizerName=Config.model_name,\n",
    "    VersionName=\"v000002\",\n",
    "    DataAccessRoleArn=Config.comprehend_iam_role,\n",
    "    InputDataConfig=dict(\n",
    "        # ----------------------------------------\n",
    "        # Define Data Format\n",
    "        # ----------------------------------------\n",
    "        DataFormat=\"COMPREHEND_CSV\",\n",
    "        # DataFormat=\"AUGMENTED_MANIFEST\",\n",
    "        # ----------------------------------------\n",
    "        # The entity types in the labeled training data that Amazon Comprehend uses to train the custom entity recognizer. Any entity types that you don’t specify are ignored.\n",
    "        #\n",
    "        # A maximum of 25 entity types can be used at one time to train an entity recognizer. Entity types must not contain the following invalid characters: n (line break), \\n (escaped line break), r (carriage return), \\r (escaped carriage return), t (tab), \\t (escaped tab), space, and , (comma).\n",
    "        # ----------------------------------------\n",
    "        EntityTypes=[\n",
    "            dict(Type=\"SERVICE_DATE\"),\n",
    "            dict(Type=\"RECEIVE_DATE\"),\n",
    "        ],\n",
    "        # ----------------------------------------\n",
    "        # The S3 location of the folder that contains the training documents for your custom entity recognizer.\n",
    "        #\n",
    "        # This parameter is required if you set DataFormat to COMPREHEND_CSV.\n",
    "        # ----------------------------------------\n",
    "        Documents=dict(\n",
    "            S3Uri=s3dir_doc_train.uri,\n",
    "            TestS3Uri=s3dir_doc_test.uri,\n",
    "            InputFormat=\"ONE_DOC_PER_FILE\",\n",
    "        ),\n",
    "        # ----------------------------------------\n",
    "        # The S3 location of the CSV file that annotates your training documents.\n",
    "        # ----------------------------------------\n",
    "        Annotations=dict(\n",
    "            S3Uri=s3path_annotation_train.uri,\n",
    "            TestS3Uri=s3path_annotation_test.uri,\n",
    "        ),\n",
    "    ),\n",
    "    LanguageCode=\"en\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}