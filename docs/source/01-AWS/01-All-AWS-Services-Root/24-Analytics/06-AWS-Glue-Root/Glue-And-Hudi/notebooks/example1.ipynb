{
 "metadata": {
  "kernelspec": {
   "name": "glue_pyspark",
   "display_name": "Glue PySpark",
   "language": "python"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sample AWS Glue Studio Notebook\n",
    "\n",
    "这个 Notebook 是一个示例, 用来演示如何在 Glue Studio Notebook 中 Interactive 的写 Hudi 代码."
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%help"
   ],
   "metadata": {
    "trusted": true,
    "editable": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare\n",
    "\n",
    "## Setup Magic\n"
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 3.0 # 到 2023-07-27 为止, notebook 对 4.0 的支持不好, 建议使用 3.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "%%configure # 必须使用以下配置才能让 spark session 使用 hudi\n",
    "{\n",
    "    \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n",
    "    \"--datalake-formats\": \"hudi\"\n",
    "}"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 2\nThe following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--datalake-formats': 'hudi'}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"myapp\")\n",
    "    .setAll(\n",
    "        [\n",
    "            # 下面两个是最重要的, 要想用 Hudi 则必须要启用这两个选项\n",
    "            (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"),\n",
    "            (\"spark.sql.hive.convertMetastoreParquet\", \"false\"),\n",
    "            # 下面的不是那么重要\n",
    "            (\"spark.sql.caseSensitive\", \"true\"),\n",
    "            (\"spark.sql.session.timeZone\", \"UTC\"),\n",
    "            (\"spark.sql.files.ignoreMissingFiles\", \"false\"),\n",
    "            (\"spark.sql.parquet.enableVectorizedReader\", \"false\"),\n",
    "            (\"spark.hadoop.hive.metastore.client.factory.class\",\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"),\n",
    "            (\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"1\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "spark_ses = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "spark_ctx = spark_ses.sparkContext\n",
    "glue_ctx = GlueContext(spark_ctx)"
   ],
   "metadata": {
    "trusted": true,
    "editable": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::807388292768:role/all-services-admin-role\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: a1ffc593-2e0d-46c6-8025-e37c3ae33bb8\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\n--datalake-formats hudi\nWaiting for session a1ffc593-2e0d-46c6-8025-e37c3ae33bb8 to get into ready status...\nSession a1ffc593-2e0d-46c6-8025-e37c3ae33bb8 has been created.\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "config = spark_ses.sparkContext.getConf()\n",
    "print(config)\n",
    "print(config.get(\"spark.sql.hive.convertMetastoreParquet\"))\n",
    "print(config.get(\"spark.serializer\"))"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "<pyspark.conf.SparkConf object at 0x7fdeebffcc90>\nfalse\norg.apache.spark.serializer.KryoSerializer\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "\n",
    "boto_ses = boto3.session.Session()\n",
    "sts_client = boto_ses.client(\"sts\")\n",
    "aws_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "aws_region = boto_ses.region_name\n",
    "\n",
    "print(f\"aws_account_id = {aws_account_id}\")\n",
    "print(f\"aws_region = {aws_region}\")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "aws_account_id = 807388292768\naws_region = us-east-1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pdf = spark_ses.createDataFrame(\n",
    "    [\n",
    "        (\"id-1\", \"2000\", \"01\", \"01\", \"2000-01-01 00:00:00\", 1),\n",
    "        (\"id-2\", \"2000\", \"01\", \"02\", \"2000-01-02 00:00:00\", 2),\n",
    "        (\"id-3\", \"2000\", \"01\", \"03\", \"2000-01-03 00:00:00\", 3),\n",
    "    ],\n",
    "    (\"id\", \"year\", \"month\", \"day\", \"ts\", \"value\"),\n",
    ")\n",
    "pdf.show()"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "+----+----+-----+---+-------------------+-----+\n|  id|year|month|day|                 ts|value|\n+----+----+-----+---+-------------------+-----+\n|id-1|2000|   01| 01|2000-01-01 00:00:00|    1|\n|id-2|2000|   01| 02|2000-01-02 00:00:00|    2|\n|id-3|2000|   01| 03|2000-01-03 00:00:00|    3|\n+----+----+-----+---+-------------------+-----+\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "database = \"mydatabase\"\n",
    "table = \"mytable\"\n",
    "additional_options={\n",
    "    \"hoodie.table.name\": table,\n",
    "    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"year,month,day\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.database\": database,\n",
    "    \"hoodie.datasource.hive_sync.table\": table,\n",
    "    \"hoodie.datasource.hive_sync.partition_fields\": \"year,month,day\",\n",
    "    \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "    \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"path\": f\"s3://{aws_account_id}-{aws_region}-data/projects/hudi-poc/databases/{database}/{table}\"\n",
    "}\n",
    "(\n",
    "    pdf.write.format(\"hudi\")\n",
    "    .options(**additional_options)\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
